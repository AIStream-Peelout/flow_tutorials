{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer-Auto-Encoder-Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzkgfvmRK11s"
      },
      "source": [
        "## Leveraging a Temporal AutoEncoder for forecasting\n",
        "\n",
        "\n",
        "In this notebook we will go over using a temporal autoencoder (a modified CustomTransformerDecoder) model for first creating representations of temporal data then for forecasting time series based on those representations. We will be training the model primarily on data from `Virgin River` which is a good example of anomalous event forecasting. Then we will train the model on additional data with layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUvaENyuKx9a",
        "outputId": "f1cd240f-9742-4e04-cad6-7ec20132cf67"
      },
      "source": [
        "from google.colab import auth\n",
        "from datetime import datetime\n",
        "import os\n",
        "auth.authenticate_user()\n",
        "!git clone https://github.com/AIStream-Peelout/flow-forecast.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'flow-forecast'...\n",
            "remote: Enumerating objects: 17363, done.\u001b[K\n",
            "remote: Counting objects: 100% (4220/4220), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1362/1362), done.\u001b[K\n",
            "remote: Total 17363 (delta 2986), reused 4023 (delta 2831), pack-reused 13143\u001b[K\n",
            "Receiving objects: 100% (17363/17363), 4.54 MiB | 6.27 MiB/s, done.\n",
            "Resolving deltas: 100% (12537/12537), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHVLW4yUK8-r",
        "outputId": "fec7a19c-db5c-4b12-be0d-037fb1e7df99"
      },
      "source": [
        "import os\n",
        "os.chdir('flow-forecast')\n",
        "!pip install shortuuid==1.0.1\n",
        "!pip install -r  requirements.txt\n",
        "!python setup.py develop\n",
        "!mkdir data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shortuuid==1.0.1\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Installing collected packages: shortuuid\n",
            "Successfully installed shortuuid-1.0.1\n",
            "Collecting shap==0.40.0\n",
            "  Downloading shap-0.40.0-cp37-cp37m-manylinux2010_x86_64.whl (564 kB)\n",
            "\u001b[K     |████████████████████████████████| 564 kB 14.1 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.3.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.10.0+cu111)\n",
            "Collecting tb-nightly\n",
            "  Downloading tb_nightly-2.9.0a20220213-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 56.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.11.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (0.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (3.1.0)\n",
            "Collecting wandb==0.12.10\n",
            "  Downloading wandb-0.12.10-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 20.3 MB/s \n",
            "\u001b[?25hCollecting google-cloud\n",
            "  Downloading google_cloud-0.34.0-py2.py3-none-any.whl (1.8 kB)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.18.1)\n",
            "Collecting plotly~=5.6.0\n",
            "  Downloading plotly-5.6.0-py2.py3-none-any.whl (27.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting pytz~=2021.3\n",
            "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 57.8 MB/s \n",
            "\u001b[?25hCollecting setuptools~=59.0.1\n",
            "  Downloading setuptools-59.0.1-py3-none-any.whl (947 kB)\n",
            "\u001b[K     |████████████████████████████████| 947 kB 61.4 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.22.2 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0rc1, 1.13.0rc2, 1.13.0, 1.13.1, 1.13.3, 1.14.0rc1, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0rc1, 1.15.0rc2, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0rc1, 1.16.0rc2, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0rc1, 1.17.0rc2, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0rc1, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0rc1, 1.19.0rc2, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0rc1, 1.20.0rc2, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0rc1, 1.21.0rc2, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for numpy==1.22.2\u001b[0m\n",
            "\u001b[?25h/usr/local/lib/python3.7/dist-packages/setuptools/dist.py:484: UserWarning: Normalizing '0.992dev' to '0.992.dev0'\n",
            "  warnings.warn(tmpl.format(**locals()))\n",
            "running develop\n",
            "running egg_info\n",
            "creating flood_forecast.egg-info\n",
            "writing flood_forecast.egg-info/PKG-INFO\n",
            "writing dependency_links to flood_forecast.egg-info/dependency_links.txt\n",
            "writing requirements to flood_forecast.egg-info/requires.txt\n",
            "writing top-level names to flood_forecast.egg-info/top_level.txt\n",
            "writing manifest file 'flood_forecast.egg-info/SOURCES.txt'\n",
            "package init file 'flood_forecast/__init__.py' not found (or not a regular file)\n",
            "package init file 'flood_forecast/transformer_xl/__init__.py' not found (or not a regular file)\n",
            "package init file 'flood_forecast/preprocessing/__init__.py' not found (or not a regular file)\n",
            "package init file 'flood_forecast/da_rnn/__init__.py' not found (or not a regular file)\n",
            "package init file 'flood_forecast/basic/__init__.py' not found (or not a regular file)\n",
            "package init file 'flood_forecast/meta_models/__init__.py' not found (or not a regular file)\n",
            "package init file 'flood_forecast/gcp_integration/__init__.py' not found (or not a regular file)\n",
            "package init file 'flood_forecast/deployment/__init__.py' not found (or not a regular file)\n",
            "package init file 'flood_forecast/custom/__init__.py' not found (or not a regular file)\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'flood_forecast.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.7/dist-packages/flood-forecast.egg-link (link to .)\n",
            "Adding flood-forecast 0.992.dev0 to easy-install.pth file\n",
            "\n",
            "Installed /content/flow-forecast\n",
            "Processing dependencies for flood-forecast==0.992.dev0\n",
            "Searching for sphinx-autodoc-typehints\n",
            "Reading https://pypi.org/simple/sphinx-autodoc-typehints/\n",
            "Downloading https://files.pythonhosted.org/packages/04/51/4b4a0740fbde50408a2f956cbb190bdc01f1c7a562627e7f102b6a30d9b4/sphinx_autodoc_typehints-1.17.0-py3-none-any.whl#sha256=081daf53077b4ae1c28347d6d858e13e63aefe3b4aacef79fd717dd60687b470\n",
            "Best match: sphinx-autodoc-typehints 1.17.0\n",
            "Processing sphinx_autodoc_typehints-1.17.0-py3-none-any.whl\n",
            "Installing sphinx_autodoc_typehints-1.17.0-py3-none-any.whl to /usr/local/lib/python3.7/dist-packages\n",
            "Adding sphinx-autodoc-typehints 1.17.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/sphinx_autodoc_typehints-1.17.0-py3.7.egg\n",
            "Searching for sphinx-rtd-theme\n",
            "Reading https://pypi.org/simple/sphinx-rtd-theme/\n",
            "Downloading https://files.pythonhosted.org/packages/e0/d2/3818e4730e314719e27f639c44164419e40eed826d63753dc480262036e8/sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl#sha256=4d35a56f4508cfee4c4fb604373ede6feae2a306731d533f409ef5c3496fdbd8\n",
            "Best match: sphinx-rtd-theme 1.0.0\n",
            "Processing sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl\n",
            "Installing sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl to /usr/local/lib/python3.7/dist-packages\n",
            "Adding sphinx-rtd-theme 1.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/sphinx_rtd_theme-1.0.0-py3.7.egg\n",
            "Searching for mpld3~=0.5\n",
            "Reading https://pypi.org/simple/mpld3/\n",
            "Downloading https://files.pythonhosted.org/packages/9e/38/3cba20b12e7e06ec82fa57a9b6cde363903350b6b54754902acf2673c46b/mpld3-0.5.7-py3-none-any.whl#sha256=3626d37da7ac11d0fce4dd5ceb37d04ba618bf951129bf6ca3f94bf48da87d6f\n",
            "Best match: mpld3 0.5.7\n",
            "Processing mpld3-0.5.7-py3-none-any.whl\n",
            "Installing mpld3-0.5.7-py3-none-any.whl to /usr/local/lib/python3.7/dist-packages\n",
            "Adding mpld3 0.5.7 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/mpld3-0.5.7-py3.7.egg\n",
            "Searching for numpy==1.22.2\n",
            "Reading https://pypi.org/simple/numpy/\n",
            "Downloading https://files.pythonhosted.org/packages/e9/6c/c0a8130fe198f27bab92f1b28631e0cc2572295f6b7a31e87efe7448aa1c/numpy-1.22.2.zip#sha256=076aee5a3763d41da6bef9565fdf3cb987606f567cd8b104aded2b38b7b47abf\n",
            "Best match: numpy 1.22.2\n",
            "Processing numpy-1.22.2.zip\n",
            "Writing /tmp/easy_install-e607lxlo/numpy-1.22.2/setup.cfg\n",
            "Running numpy-1.22.2/setup.py -q bdist_egg --dist-dir /tmp/easy_install-e607lxlo/numpy-1.22.2/egg-dist-tmp-r7atdevo\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/sandbox.py\", line 152, in save_modules\n",
            "    yield saved\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/sandbox.py\", line 193, in setup_context\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/sandbox.py\", line 254, in run_setup\n",
            "    _execfile(setup_script, ns)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/sandbox.py\", line 43, in _execfile\n",
            "    exec(code, globals, locals)\n",
            "  File \"/tmp/easy_install-e607lxlo/numpy-1.22.2/setup.py\", line 34, in <module>\n",
            "    extras_require={\n",
            "RuntimeError: Python version >= 3.8 required.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"setup.py\", line 35, in <module>\n",
            "    'dev': dev_requirements})\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/__init__.py\", line 153, in setup\n",
            "    return distutils.core.setup(**attrs)\n",
            "  File \"/usr/lib/python3.7/distutils/core.py\", line 148, in setup\n",
            "    dist.run_commands()\n",
            "  File \"/usr/lib/python3.7/distutils/dist.py\", line 966, in run_commands\n",
            "    self.run_command(cmd)\n",
            "  File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/command/develop.py\", line 34, in run\n",
            "    self.install_for_development()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/command/develop.py\", line 151, in install_for_development\n",
            "    self.process_distribution(None, self.dist, not self.no_deps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/command/easy_install.py\", line 745, in process_distribution\n",
            "    [requirement], self.local_index, self.easy_install\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 768, in resolve\n",
            "    replace_conflicting=replace_conflicting\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 1051, in best_match\n",
            "    return self.obtain(req, installer)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 1063, in obtain\n",
            "    return installer(requirement)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/command/easy_install.py\", line 669, in easy_install\n",
            "    return self.install_item(spec, dist.location, tmpdir, deps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/command/easy_install.py\", line 695, in install_item\n",
            "    dists = self.install_eggs(spec, download, tmpdir)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/command/easy_install.py\", line 890, in install_eggs\n",
            "    return self.build_and_install(setup_script, setup_base)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/command/easy_install.py\", line 1162, in build_and_install\n",
            "    self.run_setup(setup_script, setup_base, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/command/easy_install.py\", line 1146, in run_setup\n",
            "    run_setup(setup_script, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/sandbox.py\", line 257, in run_setup\n",
            "    raise\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 130, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/sandbox.py\", line 193, in setup_context\n",
            "    yield\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 130, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/sandbox.py\", line 164, in save_modules\n",
            "    saved_exc.resume()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/sandbox.py\", line 139, in resume\n",
            "    raise exc.with_traceback(self._tb)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/sandbox.py\", line 152, in save_modules\n",
            "    yield saved\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/sandbox.py\", line 193, in setup_context\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/sandbox.py\", line 254, in run_setup\n",
            "    _execfile(setup_script, ns)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/setuptools/sandbox.py\", line 43, in _execfile\n",
            "    exec(code, globals, locals)\n",
            "  File \"/tmp/easy_install-e607lxlo/numpy-1.22.2/setup.py\", line 34, in <module>\n",
            "    extras_require={\n",
            "RuntimeError: Python version >= 3.8 required.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HjF7Mtur0BM"
      },
      "source": [
        "os.environ['MODEL_BUCKET'] = \"coronaviruspublicdata\"\n",
        "os.environ[\"ENVIRONMENT_GCP\"] = \"Colab\"\n",
        "os.environ[\"GCP_PROJECT\"] = \"gmap-997\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6V_odUWK_xz",
        "outputId": "6e466f6a-2c7e-47c8-d162-ea9301dc8265"
      },
      "source": [
        "!mkdir joined_final_3\n",
        "import pandas as pd\n",
        "!gsutil cp gs://aistream-datasets/flowdb/01010500FVE_flow.csv .\n",
        "!gsutil cp gs://aistream-datasets/flowdb/09405500AZC_flow.csv .\n",
        "df = pd.read_csv(\"09405500AZC_flow.csv\")\n",
        "df = df.dropna(subset=[\"hour_updated\", \"cfs\", \"p01m\", \"tmpf\", \"dwpf\"])\n",
        "df.to_csv(\"joined_final_3/09405500AZC_flow.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://aistream-datasets/flowdb/01010500FVE_flow.csv...\n",
            "- [1 files][ 26.5 MiB/ 26.5 MiB]                                                \n",
            "Operation completed over 1 objects/26.5 MiB.                                     \n",
            "Copying gs://aistream-datasets/flowdb/09405500AZC_flow.csv...\n",
            "- [1 files][ 19.6 MiB/ 19.6 MiB]                                                \n",
            "Operation completed over 1 objects/19.6 MiB.                                     \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (2,4,7,8,9,18) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP_GQ1UuLCTz"
      },
      "source": [
        " def make_config_file():\n",
        "  run = wandb.init(project=\"auto_flow\", entity=\"autoencoder\")\n",
        "  wandb_sweep_config = wandb.config\n",
        "  the_config = {                 \n",
        "      \"model_name\": \"CustomTransformerDecoder\",\n",
        "      \"model_type\": \"PyTorch\",\n",
        "      \"model_params\": {\n",
        "        \"n_time_series\":4,\n",
        "        \"d_model\":32,\n",
        "        \"seq_length\": wandb_sweep_config[\"forecast_history\"],\n",
        "        \"output_seq_length\": wandb_sweep_config[\"forecast_history\"], \n",
        "        \"n_layers_encoder\": wandb_sweep_config[\"number_encoder_layers\"],\n",
        "        \"output_dim\":4,\n",
        "        \"squashed_embedding\": True,\n",
        "        \"use_mask\": wandb_sweep_config[\"use_mask\"]\n",
        "      }, \n",
        "      \"early_stopping\":\n",
        "      {\n",
        "          \"patience\":3\n",
        "      },\n",
        "      \"n_targets\":4,\n",
        "      \"dataset_params\":\n",
        "      {  \"class\": \"AutoEncoder\",\n",
        "        \"training_path\": \"/content/flow-forecast/joined_final_3/09405500AZC_flow.csv\",\n",
        "        \"validation_path\": \"/content/flow-forecast/joined_final_3/09405500AZC_flow.csv\",\n",
        "        \"test_path\": \"/content/flow-forecast/joined_final_3/09405500AZC_flow.csv\",\n",
        "        \"forecast_history\":wandb_sweep_config[\"forecast_history\"],\n",
        "        \"sort_column\": \"datetime\",\n",
        "        \"forecast_length\": wandb_sweep_config[\"forecast_history\"],\n",
        "        \"train_end\": 30000,\n",
        "        \"valid_start\":30001,\n",
        "        \"valid_end\": 32000,\n",
        "        \"test_start\": 32001,\n",
        "        \"test_end\": 34000,\n",
        "        \"target_col\": [\"cfs\", \"p01m\", \"tmpf\", \"dwpf\"],\n",
        "        \"relevant_cols\": [\"cfs\", \"p01m\", \"tmpf\", \"dwpf\"],\n",
        "        \"scaler\": \"StandardScaler\",\n",
        "        \"interpolate\": {\n",
        "            \"method\":\"back_forward_generic\",\n",
        "            \"params\":{\n",
        "                \"relevant_columns\":[\"cfs\",  \"p01m\", \"tmpf\", \"dwpf\"]\n",
        "            }\n",
        "            \n",
        "        }\n",
        "      },\n",
        "      \"training_params\":\n",
        "      {\n",
        "        \"criterion\":\"MSE\",\n",
        "        \"optimizer\": \"Adam\",\n",
        "        \"optim_params\":\n",
        "        {\n",
        "            \"lr\": wandb_sweep_config[\"learning_rate\"]\n",
        "\n",
        "        },\n",
        "        \"epochs\": 10,\n",
        "        \"batch_size\": wandb_sweep_config[\"batch_size\"]\n",
        "      \n",
        "      },\n",
        "      \"GCS\": True,\n",
        "      \n",
        "      \"wandb\": False,\n",
        "      \"sweep\": True,\n",
        "    \"metrics\":[\"MSE\"]\n",
        "  }\n",
        "  wandb.config.update(the_config)\n",
        "  return the_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCRIElLTp-_k"
      },
      "source": [
        "wandb_sweep_config = {\n",
        "  \"name\": \"Default sweep\",\n",
        "  \"method\": \"grid\",\n",
        "  \"parameters\": {\n",
        "        \"batch_size\": {\n",
        "            \"values\": [100, 200]\n",
        "        },\n",
        "        \"learning_rate\":{\n",
        "            \"values\":[0.001, 0.0001, .01, .00001]\n",
        "        },\n",
        "        \"forecast_history\":{\n",
        "            \"values\":[10, 24, 100]\n",
        "        },\n",
        "        \"number_encoder_layers\":\n",
        "        {\n",
        "            \"values\":[1, 3, 6]\n",
        "        },\n",
        "        \"use_mask\":{\n",
        "            \"values\":[True, False]\n",
        "        }\n",
        "    }}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "ryPN0AzpLMqL",
        "outputId": "a8ee4855-71ec-422b-f0f6-0601f8e10014"
      },
      "source": [
        "import wandb\n",
        "from flood_forecast.meta_train import train_function\n",
        "wandb.init()\n",
        "sweep_full = wandb.sweep(wandb_sweep_config, project=\"auto_flow\")\n",
        "wandb.agent(sweep_full, lambda: train_function(\"PyTorch\", make_config_file()))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2320e26d2539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflood_forecast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_train\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msweep_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msweep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwandb_sweep_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto_flow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msweep_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PyTorch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_config_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wandb'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNlzecYs21o0"
      },
      "source": [
        "!pip install wandb --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJTGgOCXO-qV"
      },
      "source": [
        "## Part II Examining Quality of Representations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJLBO0LJQLj1"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pytz\n",
        "# trained_model.test_data[\"datetime\"] = trained_model.test_data[\"datetime\"].astype(\"datetime64[ns]\")\n",
        "trained_model.test_data.get_from_start_date(datetime(2020, 3, 13))\n",
        "datetime(2020, 3, 13).astimezone(pytz.utc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vprRURELlK_5"
      },
      "source": [
        "trained_model.test_data.original_df[\"datetime\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uujw9K46exxs"
      },
      "source": [
        "trained_model.model._modules.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMWUIYCbS0h3"
      },
      "source": [
        "dt_row = trained_model.test_data.original_df[trained_model.test_data.original_df[\"datetime\"] == datetime(2020, 3, 13)]\n",
        "dt_row.index[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6eX4EBZqABh"
      },
      "source": [
        "normal_river_data = trained_model.test_data[0]\n",
        "normal_river_data1 = trained_model.test_data[200]\n",
        "normal_river_data2 = trained_model.test_data[250]\n",
        "rain_event_data = trained_model.test_data[39689]\n",
        "tn = trained_model.model.make_embedding(rain_event_data[0].unsqueeze(0))\n",
        "t1 = trained_model.model.make_embedding(normal_river_data[0].unsqueeze(0))\n",
        "t2 = trained_model.model.make_embedding(normal_river_data1[0].unsqueeze(0))\n",
        "t3 = trained_model.model.make_embedding(normal_river_data2[0].unsqueeze(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhIoOHg2qX5o"
      },
      "source": [
        "cosine_similarity(t2[:, :, 0].detach(), t3[:, :, 0].detach())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwzovHspTOmv"
      },
      "source": [
        "trained_model.test_data.original_df[trained_model.test_data.original_df[\"datetime\"].astype(\"datetime64[ns]\") > datetime(2020, 3, 10)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMuh6u5QWp-P"
      },
      "source": [
        "datetime(2020, 3, 13).replace(tzinfo=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag5SFRK8rYaT"
      },
      "source": [
        "trained_model.model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX7sk8CdrhKW"
      },
      "source": [
        "import torch\n",
        "trained_model.training.df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "732DCQqdr5YP"
      },
      "source": [
        "torch.rand(2, 5, 4) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhUFFilULaNs"
      },
      "source": [
        "## Part III Forecasting with trained auto-encoder model\n",
        "Here we will forecast with the trained auto-encoder model in step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsHiV2NvLVSd"
      },
      "source": [
        "the_config = {                 \n",
        "    \"model_name\": \"CustomTransformerDecoder\",\n",
        "    \"model_type\": \"PyTorch\",\n",
        "    \"model_params\": {\n",
        "      \"n_time_series\":4,\n",
        "      \"d_model\":32,\n",
        "      \"seq_length\":5,\n",
        "      \"output_seq_length\": 3, \n",
        "      \"n_layers_encoder\": 6,\n",
        "      \"output_dim\":1,\n",
        "      \"squashed_embedding\": True\n",
        "     }, \n",
        "    \"weight_path\": \"/content/flow-forecast/model_save/08_November_202109_00PM_model.pth\",\n",
        "    \"weight_path_add\": {\n",
        "        \"excluded_layers\": [\"output_dim_layer.weight\", \"output_dim_layer.bias\", \"out_length_lay.weight\", \"out_length_lay.bias\" ],\n",
        "        \"frozen_layers\": [\"dense_shape\", \"pe\", \"transformer_enc\", \"squashed\", \"unsquashed\"]\n",
        "    },\n",
        "    \"dataset_params\":\n",
        "    {  \"class\": \"default\",\n",
        "       \"training_path\": \"/content/flow-forecast/joined_final_3/09405500AZC_flow.csv\",\n",
        "       \"validation_path\": \"/content/flow-forecast/joined_final_3/09405500AZC_flow.csv\",\n",
        "       \"test_path\": \"/content/flow-forecast/joined_final_3/09405500AZC_flow.csv\",\n",
        "       \"forecast_history\":5,\n",
        "       \"sort_column\": \"datetime\",\n",
        "       \"forecast_length\":3,\n",
        "       \"train_end\": 19000,\n",
        "       \"valid_start\":20000,\n",
        "       \"valid_end\": 21000,\n",
        "       \"test_start\": 30000,\n",
        "       \"test_end\": 310000,\n",
        "     \n",
        "       \"target_col\": [\"cfs\"],\n",
        "       \"relevant_cols\": [\"cfs\", \"p01m\", \"tmpf\", \"dwpf\"],\n",
        "       \"scaler\": \"StandardScaler\",\n",
        "       \"interpolate\": {\n",
        "           \"method\":\"back_forward_generic\",\n",
        "           \"params\":{\n",
        "               \"relevant_columns\":[\"cfs\",  \"p01m\", \"tmpf\", \"dwpf\"]\n",
        "           }\n",
        "           \n",
        "       }\n",
        "    },\n",
        "    \"training_params\":\n",
        "    {\n",
        "       \"criterion\":\"DilateLoss\",\n",
        "       \"optimizer\": \"SGD\",\n",
        "       \"optim_params\":\n",
        "       {\n",
        "        \"lr\": 0.0001\n",
        "\n",
        "       },\n",
        "\n",
        "       \"epochs\": ,\n",
        "       \"batch_size\":100\n",
        "    \n",
        "    },\n",
        "    \"inference_params\":\n",
        "   {     \n",
        "         \"datetime_start\":\"2016-05-31\",\n",
        "          \"hours_to_forecast\":336, \n",
        "          \"test_csv_path\":\"/content/flow-forecast/joined_final_3/09405500AZC_flow.csv\",\n",
        "          \"decoder_params\":{\n",
        "            \"decoder_function\": \"simple_decode\", \n",
        "            \"unsqueeze_dim\": 1},\n",
        "         \n",
        "   },\n",
        "    \"GCS\": False,\n",
        "    \n",
        "    \"wandb\": {\n",
        "       \"name\": \"flood_forecast_auto\",\n",
        "       \"project\": \"auto_flow\",\n",
        "       \"tags\": [\"autoencoder\", \"test\"]\n",
        "    },\n",
        "   \"metrics\":[\"MSE\"]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUSOkp2MbYmI"
      },
      "source": [
        "from flood_forecast.trainer import train_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loPNGxiYbdqY"
      },
      "source": [
        "t = train_function(\"PyTorch\", the_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrd4Oe_sbhaI"
      },
      "source": [
        "from flood_forecast.deployment.inference import InferenceMode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VAvhoQza9lC"
      },
      "source": [
        "f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlTXrdUCbE5H"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_e3tp9abFHY"
      },
      "source": [
        "from flood_forecast.plot_functions import plot_df_test_with_confidence_interval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXNNmh8Gw1YP"
      },
      "source": [
        "new_shit.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3RNCKhQbImg"
      },
      "source": [
        "import plotly.graph_objects as go"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIwOOeZMbgdQ"
      },
      "source": [
        "f1 = go.Figure()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHqn4H5YbmQ_"
      },
      "source": [
        "import wandb\n",
        "import plotly\n",
        "import kaleido\n",
        "import numpy as np\n",
        "!pip install kaleido\n",
        "wandb.Image(np.asarray(plotly.io.to_image(f1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbVZ88D8r1H4"
      },
      "source": [
        "import json\n",
        "with open(\"/content/flow-forecast/model_save/04_November_202108_45PM.json\") as f1:\n",
        "  data = json.load(f1)\n",
        "a = InferenceMode(336, 20,  data, \"/content/flow-forecast/09405500AZC_flow.csv\", \"/content/flow-forecast/model_save/04_November_202108_45PM_model.pth\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJJXI6iPtEz_"
      },
      "source": [
        "a.make_plots(datetime(2020, 3, 13), wandb_plot_id=\"core1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zd-SB0rtUQm"
      },
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from flood_forecast.custom.dilate_loss import DilateLoss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GlD6CxuTF7T"
      },
      "source": [
        "from flood_forecast.time_model import PyTorchForecast\n",
        "p1 = the_config[\"dataset_params\"][\"training_path\"] \n",
        "new_shit = PyTorchForecast(\"CustomTransformerDecoder\", p1, p1, p1, the_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdT8Zw_60s33"
      },
      "source": [
        "s = SimpleLinearModel(1, 32, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuU3YdREJWur"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "laoder = DataLoader(new_shit.training, batch_size=100)\n",
        "crit = DilateLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz6YoAND2Oit"
      },
      "source": [
        "opt = Adam(s.parameters(), 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6EIlfMRsBLc"
      },
      "source": [
        "new_shit.model.out_length_lay.weight.data.uniform_(0.0, 1.0)\n",
        "new_shit.model.out_length_lay.bias.data.fill_(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RiNyTjQtjM-"
      },
      "source": [
        "class SimpleLinearModel(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A very simple baseline model to resolve some of the\n",
        "    difficulties with bugs in the various train/validation loops\n",
        "    in code. Has only two layers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, seq_length: int, n_time_series: int, output_seq_len=1, probabilistic: bool = False):\n",
        "        super().__init__()\n",
        "        self.forecast_history = seq_length\n",
        "        self.n_time_series = n_time_series\n",
        "        self.initial_layer = torch.nn.Linear(n_time_series, 1)\n",
        "        self.probabilistic = probabilistic\n",
        "        if self.probabilistic:\n",
        "            self.output_len = 2\n",
        "        else:\n",
        "            self.output_len = output_seq_len\n",
        "        self.output_layer = torch.nn.Linear(seq_length, self.output_len)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: A tensor of dimension (B, L, M) where\n",
        "        B is the batch size, L is the length of the sequence\n",
        "        \"\"\"\n",
        "        x = self.initial_layer(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.output_layer(x)\n",
        "        if self.probabilistic:\n",
        "            mean = x[..., 0][..., None]\n",
        "            std = torch.clamp(x[..., 1][..., None], min=0.01)\n",
        "            return torch.distributions.Normal(mean, std)\n",
        "        else:\n",
        "            return x.view(-1, self.output_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZGSHwypt0pI"
      },
      "source": [
        "for name, param in new_shit.model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn8afHV6nmGP"
      },
      "source": [
        "s(o)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVsXDUzfQA4U"
      },
      "source": [
        "from flood_forecast.pytorch_training import compute_loss\n",
        "crit2 = torch.nn.MSELoss()\n",
        "a = list(new_shit.model.parameters())[-6].clone()\n",
        "new_shit.model.eval()\n",
        "for src, trg in laoder:\n",
        "  opt.zero_grad()\n",
        "  o = new_shit.model.make_embedding(src)\n",
        "  print(o)\n",
        "  o = s(o.permute(0, 2, 1))\n",
        "  # print(o)\n",
        "  d = trg[:, :, 0]\n",
        "  l = compute_loss(d, o, torch.rand(2, 3, 4), crit2, False)\n",
        "  l.backward()\n",
        "  print(l.item())\n",
        "  opt.step()\n",
        "  print(new_shit.model.out_length_lay.weight.grad)\n",
        "  b = list(new_shit.model.parameters())[-6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9EsU-2d3ErG"
      },
      "source": [
        "torch.equal(new_shit.model.parameters()[-1], new_shit.model.parameters()[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs8-bwTf3Tn-"
      },
      "source": [
        "torch.equal(a, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvC5vVDrG-J0"
      },
      "source": [
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPbE34LQ4p6s"
      },
      "source": [
        "b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuOknQ_m33oP"
      },
      "source": [
        "a.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teH-bSVx33uT"
      },
      "source": [
        "b.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glqJ1QZH33z6"
      },
      "source": [
        "list(new_shit.model.named_parameters())[-6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwNBqrmHulZS"
      },
      "source": [
        "list(new_shit.model.named_parameters())[-4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiXzW8dKDO2k"
      },
      "source": [
        "new_shit.model(torch.rand(4, 5, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5T1CJIRx6rD"
      },
      "source": [
        "\n",
        "x = new_shit.model.output_dim_layer(torch.rand(5, 4, 32))\n",
        "new_shit.model.out_length_lay(x.permute(1, 2, 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhF5yEWnzLnD"
      },
      "source": [
        "x = new_shit.model.output_dim_layer(torch.rand(5, 4, 32))\n",
        "x.permute(1, 2, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4RKG-KZFzxa"
      },
      "source": [
        "new_shit.model.make_embedding(new_shit.training[200][0].unsqueeze(0)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSMURJkXDgWA"
      },
      "source": [
        "new_shit.model(new_shit.training[200][0].unsqueeze(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtV_8goJDvDy"
      },
      "source": [
        "compute_loss(torch.ones(2, 4), new_shit.training[0][1], torch.rand(2, 3), torch.nn.MSELoss(), None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3-M8c7jEZCs"
      },
      "source": [
        "new_shit.training[200][1][:, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58rFG_AqJi7J"
      },
      "source": [
        "o.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1Rvup9Kz0Sr"
      },
      "source": [
        "new_shit.training.df[\"cfs\"].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pklOPo4qTPBb"
      },
      "source": [
        "import torch\n",
        "trained_model.model(torch.rand(2, 5, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCc05C0jUA5D"
      },
      "source": [
        "t.model(torch.rand(2, 5, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekiDyGCOURhN"
      },
      "source": [
        "new_shit.model.make_embedding(torch.rand(2, 5, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "eEbQdSFRUZhD",
        "outputId": "c5a4c3ca-3a69-4347-f91c-18c4e2cdb373"
      },
      "source": [
        "trained_model.model.make_embedding(torch.rand(4, 5, 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-37f472f74d1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trained_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwKzSzispW4x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}