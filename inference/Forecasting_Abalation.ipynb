{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Forecasting_Abalation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBaZJhqx8zd9"
      },
      "source": [
        "# Abalation Testing and Inference Mode Flow Forecast\n",
        "In this tutorial we will use Flow Forecast in order to run an abalation test on some of our river flow data. We will continue studying the flow of the Virgin River though this could of course work for any data... S\n",
        "D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kzo67-sBLW1e",
        "outputId": "58f9047b-74e0-4c11-e5c6-6215dd4b39a2"
      },
      "source": [
        "from google.colab import auth\n",
        "from datetime import datetime\n",
        "import os\n",
        "auth.authenticate_user()\n",
        "!git clone https://github.com/AIStream-Peelout/flow-forecast.git\n",
        "os.environ['MODEL_BUCKET'] = \"predict_cfs\"\n",
        "os.environ[\"ENVIRONMENT_GCP\"] = \"Colab\"\n",
        "os.environ[\"GCP_PROJECT\"] = \"gmap-997\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'flow-forecast'...\n",
            "remote: Enumerating objects: 13631, done.\u001b[K\n",
            "remote: Counting objects: 100% (488/488), done.\u001b[K\n",
            "remote: Compressing objects: 100% (324/324), done.\u001b[K\n",
            "remote: Total 13631 (delta 328), reused 292 (delta 163), pack-reused 13143\u001b[K\n",
            "Receiving objects: 100% (13631/13631), 4.01 MiB | 11.45 MiB/s, done.\n",
            "Resolving deltas: 100% (9879/9879), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEbq8KfeLbb4"
      },
      "source": [
        "import os\n",
        "os.chdir('flow-forecast')\n",
        "!pip install -r  requirements.txt\n",
        "!python setup.py develop\n",
        "!mkdir data\n",
        "#!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pFMQkdFE91Vk",
        "outputId": "b8602597-100e-4a4c-d366-b6df18e802a3"
      },
      "source": [
        "import pandas as pd\n",
        "!gsutil cp gs://aistream-datasets/flowdb/09405500AZC_flow.csv .\n",
        "df = pd.read_csv(\"09405500AZC_flow.csv\")\n",
        "df = df.dropna(subset=[\"hour_updated\", \"cfs\", \"p01m\"])\n",
        "df.to_csv(\"09405500AZC_flow.cs\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying gs://aistream-datasets/flowdb/09405500AZC_flow.csv...\n",
            "- [1 files][ 19.6 MiB/ 19.6 MiB]                                                \n",
            "Operation completed over 1 objects/19.6 MiB.                                     \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (2,4,7,8,9,18) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "9mXB2DRd83kF",
        "outputId": "83d7b71a-564e-40bb-f593-431475ec5b0d"
      },
      "source": [
        "from flood_forecast.deployment.inference import InferenceMode\n",
        "import json\n",
        "!gsutil cp gs://predict_cfs/experiments/20_April_202110_41PM.json config.json\n",
        "new_water_data_path = \"09405500AZC_flow.csv\"\n",
        "weight_path = \"gs://predict_cfs/experiments/20_April_202110_41PM_model.pth\"\n",
        "# To do add support for loading config file from path (local or GCS)\n",
        "with open(\"config.json\") as y:\n",
        "  config_test = json.load(y)\n",
        "  #config_test[\"inference_params\"][\"dataset_params\"][\"relevant_cols\"] = [\"cfs1\", 'precip', \"temp\"]\n",
        "d = InferenceMode(336, 30, config_test, new_water_data_path, weight_path, \"river\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying gs://predict_cfs/experiments/20_April_202110_41PM.json...\n",
            "/ [0 files][    0.0 B/  3.0 KiB]                                                \r/ [1 files][  3.0 KiB/  3.0 KiB]                                                \r\n",
            "Operation completed over 1 objects/3.0 KiB.                                      \n",
            "gs://predict_cfs/experiments/20_April_202110_41PM_model.pth\n",
            "Blob experiments/20_April_202110_41PM_model.pth downloaded to data/predict_cfs/experiments/20_April_202110_41PM_model.pth.\n",
            "Weights sucessfully loaded\n",
            "interpolate should be below\n",
            "Running code to add temporal features\n",
            "Created datetime feature columns are: \n",
            "['cos_hour', 'sin_hour', 'cos_month', 'sin_month']\n",
            "Now loading 09405500AZC_flow.cs\n",
            "scaling now\n",
            "interpolate should be below\n",
            "Running code to add temporal features\n",
            "Created datetime feature columns are: \n",
            "['cos_hour', 'sin_hour', 'cos_month', 'sin_month']\n",
            "Now loading 09405500AZC_flow.cs\n",
            "scaling now\n",
            "interpolate should be below\n",
            "Running code to add temporal features\n",
            "Created datetime feature columns are: \n",
            "['cos_hour', 'sin_hour', 'cos_month', 'sin_month']\n",
            "Now loading 09405500AZC_flow.cs\n",
            "scaling now\n",
            "Using Wandb config:\n",
            "<function PreInitCallable.<locals>.preinit_wrapper at 0x7f0a5583a830>\n",
            "Torch is using cpu\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.27<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">05-05-04/22/21-2021_prod</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/igodfried/river\" target=\"_blank\">https://wandb.ai/igodfried/river</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/igodfried/river/runs/1m9e3cg1\" target=\"_blank\">https://wandb.ai/igodfried/river/runs/1m9e3cg1</a><br/>\n",
              "                Run data is saved locally in <code>/content/flow-forecast/wandb/run-20210422_050518-1m9e3cg1</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkDFTOF0-qyq"
      },
      "source": [
        "### Plotting a new datetime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzRv9DYN8-ad"
      },
      "source": [
        "d.make_plots(datetime(2020, 9, 20), new_water_data_path, wandb_plot_id=\"cool_plot\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQBhfeWe-p5d"
      },
      "source": [
        "d.make_plots(datetime(2020, 3, 13), new_water_data_path, wandb_plot_id=\"cool_plot2\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyWmSNYAANm8"
      },
      "source": [
        "### Testing Extreme Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MImQHq9uAS6F"
      },
      "source": [
        "# We will now grab a random slice of data (don't worry we are going to change this)\n",
        "data = d.model.training[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wagbaindA_39"
      },
      "source": [
        "sample1 = data[0][:, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK6HoksoB-Qx",
        "outputId": "f839b569-57b8-4b5f-8b98-616748346a05"
      },
      "source": [
        "\n",
        "\n",
        "sample1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([24, 8])"
            ]
          },
          "execution_count": 26,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKnGrpdlCt3D"
      },
      "source": [
        "Above we have slice of data from the train loader. Since our original list of relevant columns was `[\"cfs\",\"tmpf\",\"p01m\",\"dwpf\"]` That means index 2 will be precipitation. Lets replace this index with a really large value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4z4FmvqMCOYR",
        "outputId": "14b1b944-111e-4aad-917b-acc37e99036e"
      },
      "source": [
        "sample1[:, 2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "execution_count": 27,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skb51HmUDDAL"
      },
      "source": [
        "sample1[22, 2] = 22\n",
        "sample1[22, 2] = 25\n",
        "sample1[23, 2] = 25 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KL-4CCEDYUd",
        "outputId": "98b2736a-9cbf-450c-e503-25c36a3ab0f7"
      },
      "source": [
        "sample1[:, 2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 25., 25.])"
            ]
          },
          "execution_count": 43,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2JzsA8XDa1Q",
        "outputId": "9f902528-6ec9-4cbe-ffb8-eb84115975c8"
      },
      "source": [
        "d.model.model(data[0][:, :].unsqueeze(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/flow-forecast/flood_forecast/transformer_xl/transformer_bottleneck.py:261: UserWarning:\n",
            "\n",
            "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-0.1548,  0.0775, -0.1576, -0.1607, -0.1500]],\n",
              "       grad_fn=<SqueezeBackward1>)"
            ]
          },
          "execution_count": 44,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4alchFGDmL5",
        "outputId": "8da73b47-30d9-4028-ab0f-615bdeaa04ef"
      },
      "source": [
        "d.model.model(sample1.unsqueeze(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/flow-forecast/flood_forecast/transformer_xl/transformer_bottleneck.py:261: UserWarning:\n",
            "\n",
            "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-0.1548,  0.0775, -0.1576, -0.1607, -0.1500]],\n",
              "       grad_fn=<SqueezeBackward1>)"
            ]
          },
          "execution_count": 45,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwxC7TVgDyIP"
      },
      "source": [
        " From this we can see that the model is unfortunately not looking at precipitation at all while forecasting... FUCK FUCK FUCK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNJhiCjtEYnJ",
        "outputId": "d4c3ae8a-6105-44dd-c036-c24a0c8635e6"
      },
      "source": [
        "d.model.model(sample1.unsqueeze(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/flow-forecast/flood_forecast/transformer_xl/transformer_bottleneck.py:261: UserWarning:\n",
            "\n",
            "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-0.2228,  0.0929, -0.1725, -0.1528, -0.1757]],\n",
              "       grad_fn=<SqueezeBackward1>)"
            ]
          },
          "execution_count": 40,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    }
  ]
}